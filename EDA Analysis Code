##Nestle EDA Analysis --Sushen### 
import streamlit as st
import itertools
import pandas as pd 
import numpy as np 

from scipy.stats import pearsonr
from scipy import stats
#Visualization
import matplotlib.pyplot as plt 
import plotly.express as px
import matplotlib
import seaborn as sns
from sklearn.pipeline import Pipeline
from wordcloud import WordCloud
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from scipy.stats import chi2_contingency,chi2
import statsmodels.api as sm 
from scipy.stats import spearmanr
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.pipeline import Pipeline

from scipy.stats import anderson
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix 
from PIL import Image
# image = Image.open('cover.jpg')
matplotlib.use("Agg")


class DataFrame_Loader():

    
    def __init__(self):
        
        print("Loading DataFrame")
        
    def read_csv(self,data):
        self.df = pd.read_csv(data)
        return self.df

class EDA_Dataframe_Analysis():

    
    def __init__(self):
        
        print("General_EDA object created")

    def show_dtypes(self,x):
    	return x.dtypes


    def show_columns(self,x):
    	return x.columns


    def Show_Missing(self,x):
    	return x.isna().sum()


    def Show_Missing1(self,x):
	    return x.isna().sum()


    def Show_Missing2(self,x):
	    return x.isna().sum()


    def show_hist(self,x):
    	return x.hist()


    def Tabulation(self,x):
	    table = pd.DataFrame(x.dtypes,columns=['dtypes'])
	    table1 =pd.DataFrame(x.columns,columns=['Names'])
	    table = table.reset_index()
	    table= table.rename(columns={'index':'Name'})
	    table['No of Missing'] = x.isnull().sum().values    
	    table['No of Uniques'] = x.nunique().values
	    table['Percent of Missing'] = ((x.isnull().sum().values)/ (x.shape[0])) *100
	    table['First Observation'] = x.loc[0].values
	    table['Second Observation'] = x.loc[1].values
	    table['Third Observation'] = x.loc[2].values
	    for name in table['Name'].value_counts().index:
	        table.loc[table['Name'] == name, 'Entropy'] = round(stats.entropy(x[name].value_counts(normalize=True), base=2),2)
	    return table


    def Numerical_variables(self,x):
	    Num_var = [var for var in x.columns if x[var].dtypes!="object"]
	    Num_var = x[Num_var]
	    return Num_var

    def categorical_variables(self,x):
	    cat_var = [var for var in x.columns if x[var].dtypes=="object"]
	    cat_var = x[cat_var]
	    return cat_var

    def impute(self,x):
	    df=x.dropna()
	    return df

    def imputee(self,x):
	    df=x.dropna()
	    return df

    def Show_pearsonr(self,x,y):
	    result = pearsonr(x,y)
	    return result

	
    def Show_spearmanr(self,x,y):
	    result = spearmanr(x,y)
	    return result


    def plotly(self,a,x,y):
	    fig = px.scatter(a, x=x, y=y)
	    fig.update_traces(marker=dict(size=10,
	                                  line=dict(width=2,
	                                            color='DarkSlateGrey')),
	                      selector=dict(mode='markers'))
	    fig.show()

    def show_displot(self,x):
	        plt.figure(1)
	        plt.subplot(121)
	        sns.distplot(x)


	        plt.subplot(122)
	        x.plot.box(figsize=(16,5))

	        plt.show()

    def Show_DisPlot(self,x):
	    plt.style.use('fivethirtyeight')
	    plt.figure(figsize=(12,7))
	    return sns.distplot(x, bins = 25)

    def Show_CountPlot(self,x):
	    fig_dims = (18, 8)
	    fig, ax = plt.subplots(figsize=fig_dims)
	    return sns.countplot(x,ax=ax)

    def plotly_histogram(self,a,x,y):
	    fig = px.histogram(a, x=x, y=y)
	    fig.update_traces(marker=dict(size=10,
	                                  line=dict(width=2,
	                                            color='DarkSlateGrey')),
	                      selector=dict(mode='markers'))
	    fig.show()


    def plotly_violin(self,a,x,y):
	    fig = px.histogram(a, x=x, y=y)
	    fig.update_traces(marker=dict(size=10,
	                                  line=dict(width=2,
	                                            color='DarkSlateGrey')),
	                      selector=dict(mode='markers'))
	    fig.show()

    def Show_PairPlot(self,x):
	    return sns.pairplot(x)

    def Show_HeatMap(self,x):
	    f,ax = plt.subplots(figsize=(15, 15))
	    return sns.heatmap(x.corr(),annot=True,ax=ax);

    def wordcloud(self,x):
	    wordcloud = WordCloud(width = 1000, height = 500).generate(" ".join(x))
	    plt.imshow(wordcloud)
	    plt.axis("off")
	    return wordcloud

    def label(self,x):
	    from sklearn.preprocessing import LabelEncoder
	    le = LabelEncoder()
	    x=le.fit_transform(x)
	    return x

    def label1(self,x):
	    from sklearn.preprocessing import LabelEncoder
	    le = LabelEncoder()
	    x=le.fit_transform(x)
	    return x
   
    def concat(self,x,y,z,axis):
    	return pd.concat([x,y,z],axis)

    def dummy(self,x):
    	return pd.get_dummies(x)


    def qqplot(self,x):
    	return sm.qqplot(x, line ='45')


    def Anderson_test(self,a):
    	return anderson(a)

    def PCA(self,x):
	    pca =PCA(n_components=8)
	    principlecomponents = pca.fit_transform(x)
	    principledf = pd.DataFrame(data = principlecomponents)
	    return principledf

    def outlier(self,x):
	    high=0
	    q1 = x.quantile(.25)
	    q3 = x.quantile(.75)
	    iqr = q3-q1
	    low = q1-1.5*iqr
	    high += q3+1.5*iqr
	    outlier = (x.loc[(x < low) | (x > high)])
	    return(outlier)



    def check_cat_relation(self,x,y,confidence_interval):
	    cross_table = pd.crosstab(x,y,margins=True)
	    stat,p,dof,expected = chi2_contingency(cross_table)
	    print("Chi_Square Value = {0}".format(stat))
	    print("P-Value = {0}".format(p))
	    alpha = 1 - confidence_interval
	    return p,alpha
	    if p > alpha:
	        print(">> Accepting Null Hypothesis <<")
	        print("There is no relationship between the two variables")
	    else:
	        print(">> Rejecting Null Hypothesis <<")
	        print("There is a significant relationship between the two variables")



class Attribute_Information():

    def __init__(self):
        
        print("Attribute Information object created")
        
    def Column_information(self,data):
    
        data_info = pd.DataFrame(
                                columns=['No of observations',
                                        'No of Variables',
                                        'No of Numerical Variables',
                                        'No of Factor Variables',
                                        'No of Categorical Variables',
                                        'No of Logical Variables',
                                        'No of Date Variables',
                                        'No of zero variance variables'])


        data_info.loc[0,'No of observations'] = data.shape[0]
        data_info.loc[0,'No of Variables'] = data.shape[1]
        data_info.loc[0,'No of Numerical Variables'] = data._get_numeric_data().shape[1]
        data_info.loc[0,'No of Factor Variables'] = data.select_dtypes(include='category').shape[1]
        data_info.loc[0,'No of Logical Variables'] = data.select_dtypes(include='bool').shape[1]
        data_info.loc[0,'No of Categorical Variables'] = data.select_dtypes(include='object').shape[1]
        data_info.loc[0,'No of Date Variables'] = data.select_dtypes(include='datetime64').shape[1]
        data_info.loc[0,'No of zero variance variables'] = data.loc[:,data.apply(pd.Series.nunique)==1].shape[1]

        data_info =data_info.transpose()
        data_info.columns=['value']
        data_info['value'] = data_info['value'].astype(int)


        return data_info

    def __get_missing_values(self,data):
        
        #Getting sum of missing values for each feature
        missing_values = data.isnull().sum()
        #Feature missing values are sorted from few to many
        missing_values.sort_values(ascending=False, inplace=True)
        
        #Returning missing values
        return missing_values

        
    def __iqr(self,x):
        return x.quantile(q=0.75) - x.quantile(q=0.25)

    def __outlier_count(self,x):
        upper_out = x.quantile(q=0.75) + 1.5 * self.__iqr(x)
        lower_out = x.quantile(q=0.25) - 1.5 * self.__iqr(x)
        return len(x[x > upper_out]) + len(x[x < lower_out])

    def num_count_summary(self,df):
        df_num = df._get_numeric_data()
        data_info_num = pd.DataFrame()
        i=0
        for c in  df_num.columns:
            data_info_num.loc[c,'Negative values count']= df_num[df_num[c]<0].shape[0]
            data_info_num.loc[c,'Positive values count']= df_num[df_num[c]>0].shape[0]
            data_info_num.loc[c,'Zero count']= df_num[df_num[c]==0].shape[0]
            data_info_num.loc[c,'Unique count']= len(df_num[c].unique())
            data_info_num.loc[c,'Negative Infinity count']= df_num[df_num[c]== -np.inf].shape[0]
            data_info_num.loc[c,'Positive Infinity count']= df_num[df_num[c]== np.inf].shape[0]
            data_info_num.loc[c,'Missing Percentage']= df_num[df_num[c].isnull()].shape[0]/ df_num.shape[0]
            data_info_num.loc[c,'Count of outliers']= self.__outlier_count(df_num[c])
            i = i+1
        return data_info_num
    
    def statistical_summary(self,df):
    
        df_num = df._get_numeric_data()

        data_stat_num = pd.DataFrame()

        try:
            data_stat_num = pd.concat([df_num.describe().transpose(),
                                       pd.DataFrame(df_num.quantile(q=0.10)),
                                       pd.DataFrame(df_num.quantile(q=0.90)),
                                       pd.DataFrame(df_num.quantile(q=0.95))],axis=1)
            data_stat_num.columns = ['count','mean','std','min','25%','50%','75%','max','10%','90%','95%']
        except:
            pass

        return data_stat_num



class Data_Base_Modelling():

    
    def __init__(self):
        
        print("General_EDA object created")


    def Label_Encoding(self,x):
	    category_col =[var for var in x.columns if x[var].dtypes =="object"] 
	    labelEncoder = preprocessing.LabelEncoder()
	    mapping_dict={}
	    for col in category_col:
	        x[col] = labelEncoder.fit_transform(x[col])
	        le_name_mapping = dict(zip(labelEncoder.classes_, labelEncoder.transform(labelEncoder.classes_)))
	        mapping_dict[col]=le_name_mapping
	    return mapping_dict

    def IMpupter(self,x):
	    imp_mean = IterativeImputer(random_state=0)
	    x = imp_mean.fit_transform(x)
	    x = pd.DataFrame(x)
	    return x

    ##def variancechecker()
    
1) Remove features with low -variance 

The first feature elimination method which we could use is to remove features with low variance. 
The thinking behind this is that features which have low variance, i.e. features which mostly remain at the same level across different observations, 
should not ideally be responsible for differing responses in the observations.   

Select X as the set of all features, and Y as the repsonse variable. Here, the response variable Y is class. 

X = df_diabetes[['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']]
Y = df_diabetes['class']
We now create a VarianceThreshold object of Sklearn, with a variance threshold of 0.3 (i.e. remove features with variance less than 30%). 
Next we fit the VarianceThreshold object with the response variable X and the feature matrix Y.

from sklearn.feature_selection import VarianceThreshold

var = VarianceThreshold(threshold=0.3)
var = var.fit(X,Y)
With the .get_support method of sklearn classification objects we see that the feature with index 6 in the feature matrix has been eliminated. 

cols = var.get_support(indices=True)
cols
#Output
array([0, 1, 2, 3, 4, 5, 7], dtype=int64)
If we now look at the feature names, we see that the feature "pedi" has been eliminated from the feature matrix.

features = X.columns[cols]
features
#Output
Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'age'], dtype='object')
    
